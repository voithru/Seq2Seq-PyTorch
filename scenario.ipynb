{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# attention based seq2seq 로 summarization 구현\n",
    "이상헌 <br>\n",
    "voithru<br>\n",
    "\n",
    "## 0. dataset 구현\n",
    "\n",
    "마땅히 알맞은 데이터 셋을 찾지 못하다가 <br>\n",
    "\n",
    "딥마인드의 [해당 논문](https://arxiv.org/abs/1506.03340) 에서 사용한 데이터셋은 **DMQA** 이다. <br>\n",
    "\n",
    "여기서는 QnA의 모델링으로 사용하였는데, 해당 파일 안에 summary도 있나보다. <br>\n",
    "\n",
    "### 0.1. 다운로드 및 밑작업\n",
    "일단 다운로드를 시작한다. <br>\n",
    "다음과 같은 [위치](http://cs.nyu.edu/~kcho/DMQA//)에서 받는다. <br>\n",
    "```\n",
    "CNN_stories.tgz\n",
    "dailymail_stories.tgz\n",
    "```\n",
    "now unzip it <br>\n",
    "\n",
    "이 안에는 @highlight로 emphasis 되어있는데, 이게 summary 라고 하는 것이다. <br>\n",
    "\n",
    "밑작업을 위해서는 해당 데이터를 **tokenize**, 그리고 **binary** 로 만들어야 할 필요가 있다. <br>\n",
    "\n",
    "cf) <br>\n",
    "tokenize는 무엇인가. <br>\n",
    "![tokenize 설명 img](https://cloud.githubusercontent.com/assets/2272790/18410099/1d0a1c1a-7761-11e6-9fe1-bd2e5622b90a.png)\n",
    "* GO - ```<start>``` token. decoder에 가장 첫 번째 node에 들어갈 token.\n",
    "* EOS - ```<end>``` token.\n",
    "* UNK - unkown token. vocab에 들어있지 않는 rare vocab을 replace하기 위한 token. 우리 seq2seq에서는 사용되지 않는다! 하지만 굉장히 더러운 data에 있어서는 사용해야 한다. 예) ```my name is skdy33``` => ```my name is _unk_```\n",
    "* PAD - mini-batch 안에 들어있는 데이터는 같은 길이를 가져야 한다. 따라서 짧은 데이터는 ```_pad_``` 토큰이 뒤에 붙는다.\n",
    "\n",
    "\n",
    "### 참고자료\n",
    "* [google's text summarization code](https://github.com/pranay360/TextSum_Data_Generation)\n",
    "* [data-generation code](https://github.com/pranay360/TextSum_Data_Generation)\n",
    "* [Get to the point : summarization with  pointer-generator networks](https://arxiv.org/pdf/1704.04368.pdf)\n",
    "* [what does PAD / GO / EOS / UNK mean?](https://github.com/nicolas-ivanov/tf_seq2seq_chatbot/issues/15)\n",
    "* [utf8 and encoding](wholetext=re.sub(r'[^\\x00-\\x7F]+','', wholetext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data conversion 시작\n",
    "\n",
    "# 가장 먼저 nltk 설치하고 오세요\n",
    "# sudo apt-get install python-nltk\n",
    "# pip install nltk\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import struct\n",
    "import numpy as np\n",
    "import collections\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tensorflow.core.example import example_pb2 # why do we need this\n",
    "import struct # 이건 또 어떻게 사용하는 거지. https://docs.python.org/3/library/struct.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/voithru/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 또한 punkt corpus를 필요로 하는데, 왜 필요로 하는지는 모르겠지만 설치하자.\n",
    "# 실제로 문장을 어디서 자를 지, 미리 학습된 tokenizer.\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 목적\n",
    "\n",
    "일단 string을 binary로 만들어야 함은 자명하다. <br>\n",
    "이러한 핸들링을 위하여 tensorflow는 ```from tensorflow.core.example import example_pb2``` 를 제공한다. <br>\n",
    "\n",
    "본 형태는 다음과 같다. \n",
    "```python\n",
    "features {\n",
    "  feature {\n",
    "    key: \"abstract\"\n",
    "    value {\n",
    "      bytes_list {\n",
    "        value: \"<d> <p> <s> president obama has moved this month \\'s g - 8 summit from chicago to camp david </s> </p> </d>\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  feature {\n",
    "    key: \"article\"\n",
    "    value {\n",
    "      bytes_list {\n",
    "        value: \"<d> <p> <s> ( cnn ) - - when the white house , back in march , made the unexpected announcement that this month \\'s g - 8 summit , long planned to take place in chicago , was being moved to camp david , the reasons given were bland: camp david was taking over from chicago \\\" to facilitate a free - flowing discussion with our close g - 8 partners , \\\" the white house announced . </s> <s> president obama was said to believe that camp david was a more intimate setting for the world leaders to get together . </s> <s> there was immediate conversation about the explanation . </s> <s> past g - 8 summits have been known to attract protests that culminated in violence in the streets of the cities where they have been held . </s> <s> camp david - - the presidential retreat in catoctin mountain park in frederick county , maryland - - is about as secure a sealed - off location as exists in this country . </s> <s> no one gets near it who is not supposed to get near it . </s> <s> originally the chicago g - 8 summit was supposed to be immediately followed in chicago by a separate summit of the leaders of nato nations . </s> <s> the nato meeting is still scheduled for chicago; like the g - 8 meeting , it will be hosted by obama . </s> <s> demonstrations are planned , and heavy security will be in place with the intention of keeping protesters well away from where the nato leaders are convening . </s> <s> increasingly , over the years , cities that engage in grand talk about \\\" bringing the world \\\" to see their many civic and cultural attributes end up going into a defensive crouch at the same time they are welcoming their global visitors . </s> </p> </d>\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "여기서 ```<d>,<p>,<s>``` 등의 tag를 보자. <br>\n",
    "* ```<d>``` 의 경우는 document를 의미한다.\n",
    "* ```<p>``` 의 경우는 paragrapgh를 의미한다.\n",
    "* ```<s>``` 의 경우는 sentence를 의미한다.\n",
    "\n",
    "여기서 큰 문제가 발생한다. <br>\n",
    "\n",
    "## period가 없으면 sentence 단위로 문장을 끊는 것이 매우매우 어렵다.\n",
    "실제로 nltk의 sent_tokenizer는 ```PunktSentenceTokenizer``` 라는 것을 사용하는데, <br>\n",
    "이는 unsupervised training model로 어디서 문장을 끊을 지 학습한다. <br>\n",
    "\n",
    "혹시 이를 학습시켜서 punctuation이 없는 도레에도 splitting을 시킬 수 있지 않을까?\n",
    "\n",
    "* [how to split long sentences](http://www.aje.com/en/arc/editing-tip-strategies-splitting-long-sentences/)\n",
    "* [PunktSentenceTokenizer](http://nlpforhackers.io/splitting-text-into-sentences/)\n",
    "* [case1](https://kheafield.com/code/kenlm/)\n",
    "* [case2](https://code.google.com/archive/p/berkeleylm/)\n",
    "* [case3](http://www.speech.sri.com/projects/srilm/)\n",
    "\n",
    "### 일단 이 issue 는 클로즈 한다. 영진, 종민에게 토스.\n",
    "\n",
    "### 일단 CNN으로만 진행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92579\n"
     ]
    }
   ],
   "source": [
    "# word dictionary를 위한 counter 객체\n",
    "counter = collections.Counter()\n",
    "temp=0\n",
    "#train, test, validation split\n",
    "tr_r=0.85\n",
    "v_r=0.08\n",
    "# directory list\n",
    "files = os.listdir('data/cnn/stories/')\n",
    "# file 수\n",
    "n_files = len(files)\n",
    "print(n_files)\n",
    "# train\n",
    "train=files[:int(n_files*0.8)]\n",
    "validation=files[len(train):len(train)+int(n_files*0.12)]\n",
    "test=files[len(train)+len(validation):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(cnn) -- the first anniversary of the egyptian revolution is today. egyptian society and the forces in egypt are in a state of anticipation. world media has its cameras and correspondents in cairo and major cities around the country. but many egyptians wonder if the revolution amounted to nothing more than a military coup.\n",
      "\n",
      "it has been a year since the eruption of the first egyptian revolution that stunned the world and ended 30 years of authoritarian, oppressive and corrupt rule by hosni mubarak, egypt's last pharaoh. mubarak ended up in jail along with his sons and his regime's major figures, with stories of their unimaginable corruption, brutality and looting surfacing ever since.\n",
      "\n",
      "but since then, most egyptians have become angry and frustrated with the performance of the military council -- comprising more than a dozen elderly generals -- that has taken control. egyptians, political forces and revolutionaries accuse the military council of being accomplices with the remnants of the mubarak regime and of refusing to enact real reform, to the point of actually waging a counter-revolution.\n",
      "\n",
      "egyptians have been enraged by the brutal suppression of peaceful demonstrations that followed the revolution, and in the arrests, humiliations and even killings of protesters. the military council is still controlling the state media and the political scene. western governments and human rights organizations are expressing their dismay, with human rights watch reporting killings and brutal attacks, some of them sexual, on protesters.\n",
      "\n",
      "the council is the old guard, desperately trying to preserve the military's long-standing privileges and special status. the military has been running egypt since 1952 -- with rulers such as gen. mohammed naguib, gamal abdel nasser, anwar sadat, mubarak and marshal tantawy coming from its ranks. the military has dominated egypt's modern political scene, with members serving as presidents, prime ministers, cabinet ministers, governors, party elites, heads of the security services, ambassadors, heads of many companies, and even ministers of culture and the media -- in a true orwellian fashion.\n",
      "\n",
      "the military institution in egypt consumes more than 25% of the government expenditure and owns about 30% of the national economy. the united states has given egypt a hefty military aid package of billions of dollars for nearly three decades. egyptians respect their armed forces, but are desperate for a civil state where human rights, rights of minorities, and accountability and transparency are guaranteed. they do not want another pharaoh-like president coming from that institution.\n",
      "\n",
      "egyptians have been asked to commemorate the special occasion of the revolution's first anniversary by taking to the streets wednesday, wearing black in mourning of the deaths of hundreds of the revolution's martyrs. but some political forces refuse to call it a celebration -- they say the revolution is not finished and their demands have not been met.\n",
      "\n",
      "demonstrators have already started to pour into tahrir square. activists are calling for the immediate transition of power from the military council to a civilian council, or even an interim caretaker. now, after bringing down the mubarak regime and his notorious state security apparatus, egyptians have broken the fear barrier and believe there is no turning back. will they stun the world again, with egyptian revolution, part ii?\n",
      "\n",
      "follow cnn opinion on twitter.\n",
      "\n",
      "join the conversation on facebook.\n",
      "\n",
      "the opinions expressed in this commentary are solely those of aladdin elaasar.\n",
      "\n",
      "@highlight\n",
      "\n",
      "wednesday marks one year since the revolution ending 30 years of mubarak regime\n",
      "\n",
      "@highlight\n",
      "\n",
      "aladdin elaasar: egyptians are bitter about military council's own oppressive rule since then\n",
      "\n",
      "@highlight\n",
      "\n",
      "military has brutally repressed demonstrations, he writes; arrested, even killed protesters\n",
      "\n",
      "@highlight\n",
      "\n",
      "elaasar: activists now want the immediate transfer of power to a civilian council\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text2bin1(docs, writer):\n",
    "    \"\"\"\n",
    "    CNN의 데이터를 <s>, <p>, <d> 의 알맞은 태그와 여러 input을 고정시켜주는 함수.\n",
    "    input : \n",
    "        docs : list. list of raw data files\n",
    "        writer : open buffer\n",
    "    \"\"\"\n",
    "    global counter\n",
    "    for i, fi in enumerate(docs):\n",
    "        with open(\"data/cnn/stories/\" + files[0],'r') as f:\n",
    "            # decode 맞춰야 하나?\n",
    "            txt = f.read().lower()\n",
    "            # delete non-ascii\n",
    "            txt = re.sub(r'[^\\x00-\\x7F]+','',txt)\n",
    "            # '안녕' 을 ' 안녕 ' 으로 만든다.\n",
    "            txt=re.sub(r\"(\\s?[\\']\\s+|\\s+[\\']\\s?)\",\" ' \", txt)\n",
    "            # \"안녕\" 을 \" 안녕 \" 으로 만든다.\n",
    "            wholetext=re.sub(r'(\\s?[\\\"]\\s+|\\s+[\\\"]\\s?)',' \" ', txt)\n",
    "            # asdf's 를 asdf 's 로 바꿈\n",
    "            wholetext=re.sub(r\"(\\'[s]\\s+)\",\" 's \", wholetext)\n",
    "            wholetext=wholetext.replace(\".\",\" . \")\n",
    "            wholetext=wholetext.replace(\",\",\" , \")\n",
    "            wholetext=wholetext.replace('-',' - ')\n",
    "            wholetext=wholetext.replace('?',' ? ')\n",
    "            wholetext=wholetext.replace('(','( ')\n",
    "            wholetext=wholetext.replace(')',' )')\n",
    "            data=wholetext.split(\"@highlight\")\n",
    "            # http://pavel.surmenok.com/2016/10/15/how-to-run-text-summarization-with-tensorflow/\n",
    "            # 여기서 example_pb2 가 뭔지 잘 이해해야 할 것 같다.\n",
    "            # 그리고 여기서는 하나의 highlight만을 사용한다. 왜 하나의 highlight만을 사용하지?\n",
    "            news=data[0]\n",
    "            highlights=data[1].replace('\\n\\n','')\n",
    "            news=(\" \".join(news.split('\\n\\n'))).strip()\n",
    "            sentences = sent_tokenize(news) # 여기서 tokenize는 \n",
    "            news = '<d> <p> ' + ' '.join(['<s> ' + sentence + ' </s>' for sentence in sentences]) + ' </p> </d>'\n",
    "            highlights = '<d> <p> <s> ' + highlights + ' </s> </p> </d>'\n",
    "            words = (news+\" \"+highlights).split()\n",
    "            counter.update(words)\n",
    "            tf_example = example_pb2.Example()\n",
    "            tf_example.features.feature['article'].bytes_list.value.extend([(' '.join(news.split())).encode('utf-8')])\n",
    "            tf_example.features.feature['abstract'].bytes_list.value.extend([(' '.join(highlights.split())).encode('utf-8')])\n",
    "            tf_example_str = tf_example.SerializeToString() # 이게 format을 binary 화 한 것.\n",
    "            str_len = len(tf_example_str)\n",
    "            writer.write(struct.pack('q', str_len))\n",
    "            writer.write(struct.pack('%ds' % str_len, tf_example_str)) # this means str-len length string.\n",
    "            if i%3000 == 0:\n",
    "                print(int((float(i)/ len(docs))*100), \"%\")\n",
    "    print((float(len(docs))/ len(docs))*100, \"%....\" \"converted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 %\n",
      "4 %\n",
      "8 %\n",
      "12 %\n",
      "16 %\n",
      "20 %\n",
      "24 %\n",
      "28 %\n",
      "32 %\n",
      "36 %\n",
      "40 %\n",
      "44 %\n",
      "48 %\n",
      "52 %\n",
      "56 %\n",
      "60 %\n",
      "64 %\n",
      "68 %\n",
      "72 %\n",
      "76 %\n",
      "81 %\n",
      "85 %\n",
      "89 %\n",
      "93 %\n",
      "97 %\n",
      "100.0 %....converted\n"
     ]
    }
   ],
   "source": [
    "with open('data/trainCNN.bin','wb') as writer:\n",
    "    convert_text2bin1(train,writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 %\n",
      "40 %\n",
      "81 %\n",
      "100.0 %....converted\n",
      "0 %\n",
      "27 %\n",
      "54 %\n",
      "81 %\n",
      "100.0 %....converted\n"
     ]
    }
   ],
   "source": [
    "with open('data/testCNN.bin','wb') as writer:\n",
    "    convert_text2bin1(test,writer)\n",
    "with open('data/validateCNN.bin','wb') as writer:\n",
    "    convert_text2bin1(validation,writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이제 vocabulary를 파일로 만들어서 넣어야 한다. 여기서 갯수는 200000-2 로 한 이유는 잘 모르겠고\n",
    "# 또한 이 데이터는 굳이 binary로 할 이유가 없다.\n",
    "mc = counter.most_common(200000-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/vocab\", 'w') as writer:\n",
    "    for word, count in mc:\n",
    "        writer.write(word + \" \" + str(count) + '\\n')\n",
    "    writer.write('<UNK> 0\\n') # 언노운 태그\n",
    "    writer.write('<PAD> 0\\n') # 패딩 tag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이제 논문구현\n",
    "\n",
    "현재 구글 [github 파일](https://github.com/tensorflow/models/tree/master/research/textsum)에서는\n",
    "* build 는 bazel로\n",
    "* [data batcher](https://github.com/tensorflow/models/blob/master/research/textsum/data.py)\n",
    "* [batch reader](https://github.com/tensorflow/models/blob/master/research/textsum/batch_reader.py)\n",
    "* [beam search](https://github.com/tensorflow/models/blob/master/research/textsum/beam_search.py)\n",
    "* [seq2seq-attention model](https://github.com/tensorflow/models/blob/master/research/textsum/seq2seq_attention_model.py)\n",
    "* [seq2seq-attention decoding](https://github.com/tensorflow/models/blob/master/research/textsum/seq2seq_attention_decode.py)\n",
    "* [seq2seq-attention](https://github.com/tensorflow/models/blob/master/research/textsum/seq2seq_attention.py)\n",
    "* [seq2seq-attention-lib](https://github.com/tensorflow/models/blob/master/research/textsum/seq2seq_lib.py)\n",
    "\n",
    "으로 나뉘어 있다. <br>\n",
    "\n",
    "먼저 스키밍을 통하여 각자의 역할을 알아보도록 한다."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
