{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# attention based seq2seq 로 summarization 구현\n",
    "이상헌 <br>\n",
    "voithru<br>\n",
    "\n",
    "## 0. dataset 구현\n",
    "\n",
    "마땅히 알맞은 데이터 셋을 찾지 못하다가 <br>\n",
    "\n",
    "딥마인드의 [해당 논문](https://arxiv.org/abs/1506.03340) 에서 사용한 데이터셋은 **DMQA** 이다. <br>\n",
    "\n",
    "여기서는 QnA의 모델링으로 사용하였는데, 해당 파일 안에 summary도 있나보다. <br>\n",
    "\n",
    "### 0.1. 다운로드 및 밑작업\n",
    "일단 다운로드를 시작한다. <br>\n",
    "다음과 같은 [위치](http://cs.nyu.edu/~kcho/DMQA//)에서 받는다. <br>\n",
    "```\n",
    "CNN_stories.tgz\n",
    "dailymail_stories.tgz\n",
    "```\n",
    "now unzip it <br>\n",
    "\n",
    "이 안에는 @highlight로 emphasis 되어있는데, 이게 summary 라고 하는 것이다. <br>\n",
    "\n",
    "밑작업을 위해서는 해당 데이터를 **tokenize**, 그리고 **binary** 로 만들어야 할 필요가 있다. <br>\n",
    "\n",
    "cf) <br>\n",
    "tokenize는 무엇인가. <br>\n",
    "![tokenize 설명 img](https://cloud.githubusercontent.com/assets/2272790/18410099/1d0a1c1a-7761-11e6-9fe1-bd2e5622b90a.png)\n",
    "* GO - ```<start>``` token. decoder에 가장 첫 번째 node에 들어갈 token.\n",
    "* EOS - ```<end>``` token.\n",
    "* UNK - unkown token. vocab에 들어있지 않는 rare vocab을 replace하기 위한 token. 우리 seq2seq에서는 사용되지 않는다! 하지만 굉장히 더러운 data에 있어서는 사용해야 한다. 예) ```my name is skdy33``` => ```my name is _unk_```\n",
    "* PAD - mini-batch 안에 들어있는 데이터는 같은 길이를 가져야 한다. 따라서 짧은 데이터는 ```_pad_``` 토큰이 뒤에 붙는다.\n",
    "\n",
    "\n",
    "\n",
    "### 참고자료\n",
    "* [google's text summarization code](https://github.com/pranay360/TextSum_Data_Generation)\n",
    "* [data-generation code](https://github.com/pranay360/TextSum_Data_Generation)\n",
    "* [Get to the point : summarization with  pointer-generator networks](https://arxiv.org/pdf/1704.04368.pdf)\n",
    "* [what does PAD / GO / EOS / UNK mean?](https://github.com/nicolas-ivanov/tf_seq2seq_chatbot/issues/15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data conversion 시작\n",
    "\n",
    "# 가장 먼저 nltk 설치\n",
    "# sudo apt-get install python-nltk\n",
    "# pip install nltk\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import struct\n",
    "import numpy as np\n",
    "import collections\n",
    "from nltk.tokenize import sent_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word dictionary를 위한 counter 객체\n",
    "counter = collections.Counter()\n",
    "temp=0\n",
    "#train, test, validation split\n",
    "tr_r=0.85\n",
    "v_r=0.08\n",
    "# directory list\n",
    "files_cnn = os.listdir('data/cnn/stories/')\n",
    "files_daily = os.listdir('data/dailymail/stories/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
